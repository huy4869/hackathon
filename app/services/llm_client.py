from llama_cpp import Llama
import os

# Ch·ªâ load 1 l·∫ßn
from pathlib import Path

# ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn m√¥ h√¨nh, b·∫•t k·ªÉ file ƒëang ·ªü ƒë√¢u
LLM_PATH = str(Path(__file__).resolve().parents[2] / "models" / "gemma-3n-e4b-it.gguf")

print("üëâ Loading model from:", LLM_PATH)
assert Path(LLM_PATH).exists(), f"‚ùå Model file not found: {LLM_PATH}"


# LLM_PATH = "models/gemma-3n-e4b-it.gguf"


llm = Llama(
    # model_path=LLM_PATH,    
    # n_ctx=2048,
    # n_threads=6,
    # n_gpu_layers=20,  # t√πy m√°y, b·∫°n c√≥ th·ªÉ gi·∫£m n·∫øu RAM th·∫•p
    # verbose=False
    model_path=LLM_PATH,
    n_ctx=2048,
    n_threads=6,
    n_gpu_layers=20,
    use_mmap=True,
    use_mlock=False,
    batch_size=1024,   # ho·∫∑c 512 t√πy RAM
    verbose=False
)

def build_prompt(context: str, question: str) -> str:
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh. D·ª±a tr√™n ng·ªØ c·∫£nh b√™n d∆∞·ªõi, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ng·∫Øn g·ªçn v√† t·ª± nhi√™n nh∆∞ con ng∆∞·ªùi. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y tr·∫£ l·ªùi "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu".

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {question}
Tr·∫£ l·ªùi:"""
    return prompt

def ask_llm(context: str, question: str) -> str:
    prompt = build_prompt(context, question)
    response = llm(
        prompt,
        max_tokens=512,
        temperature=0.2,
        top_p=0.95,
        stop=["C√¢u h·ªèi:", "Ng·ªØ c·∫£nh:"]
    )
    return response["choices"][0]["text"].strip()


